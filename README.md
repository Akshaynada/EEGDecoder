# EEGDecoder
Used Deep neural networks to decode EEG signals to movement-related information

Problem description 
-------------------
The goal of this project was to decode EEG signals into movement-related information through deep learning architectures. Various architectures were investigated using Keras that utilized convolutional and recurrent layers. Through extensive testing, we reached the conclusion that a mixture of convolutional and recurrent layers performed the best and that preprocessing the raw EEG signals was crucial for performance. 

Introduction
------------

Upon tackling the problem, our first idea was to solely use recurrent layers due to the temporal nature of the data. As discussed in class, traditional deep neural nets like convolutional nets and fully connected natures cannot capture temporal relationships in data. As EEG is a time series, we thought it would be tremendously helpful to consider the time sequence information of the signal. However, upon testing using only recurrent layer, our performance was poor as can be seen in Figure x on page x. Our testing for recurrent layers consisted of first using single layer GRUs and LSTMs. The paper "Speech Recognition with Deep Recurrent Neural Networks" by Graves where it was found that the depth of the RNN was more important than the number of cells led us to try stacking recurrent layers. However, the results were still middling and further research and literature review affirmed our belief that RNNs were not the best algorithm for this task as there were few research papers applying RNNs to EEG decoding. On the other hand, there were a plethora of papers that applied deep convolutional networks to the task. Our approach then shifted to focusing on deep convolutional neural network architectures. However, we still wanted to model the recurrent connections that are an inherent part of biological neurons. CNNs are a feed-forward architecture but the time series nature of EEG signals motivated us to try an architecture with a CNN layer followed by an RNN layer. In particular, the paper, "Learning Representations from EEG with Deep-Recurrent-Convolutional Neural Networks," by Dr. Bashivan of the University of Memphis served as an inspiration as tried to design our hybrid architecture. The reasoning behind hybrid architectures is that the ConvNets can deal with variations in space and frequency domains due to their ability to learn two-dimensional representations of the data. These extracted representations could then be fed into a recurrent layer to account for the temporal variations in the data and appropriately model the temporal evolution of brain activity. For the recurrent layer, we decided to use an LSTM as it is the best at capturing long-term dependencies and performed better than GRUs in our testing. The best model we came up with is shown below. It consists of a 1D Convolution with 256 filters of size 3, Max Pooling of size 3, two LSTM layers of 64 units, Gaussian Noise, Dropout of 0.5, and a fully connected output layer of four units with softmax activation. The Gaussian Noise and dropout were added to prevent overfitting by adding robustness to the model while the max pooling was used for down sampling and reducing the number of parameters. 


![alt text](https://github.com/Akshaynada/EEGDecoder/blob/master/Codebase/Model%20architecture.png)

Results
-------
The results of our experiments showed that hybrid architectures performed the best. To train our algorithm, we concatenated all 9 subjects' datasets to train our network across all subjects. Our test set consisted of 50 trials from each subject. As mentioned in the introduction, we first used single layer LSTMs and GRUs and their performance was poor as can be seen in Figure x on page x. We then tried stacking them and performance slightly improved but not to an acceptable level. This is when we switched to using convolutional neural networks and hybrid architectures. As can be seen in Figure 2 on page x, using a 1D convolutional layer followed by two LSTM layers performed well. It achieved a result of 88% training accuracy, 62% validation accuracy, and 64% testing accuracy.


Conclusions
-----------
A major insight we gained from this project is that preprocessing is key. At first, we applied the standard mean normalization to the raw ECG data without doing anything else. Our performance was poor, and we were not sure at first whether it was due to our architecture or other factors. Upon trying more architectures and seeing the same results, we started to look up preprocessing steps other researchers have done on raw EEG data. We concluded that preprocessing is a very important part in decoding ECG signals. The benefits of preprocessing raw EEG signals are that it increases the signal to noise ratio, enhancing the relevant information that is entrenched in the signal. Since EEG signals are easily affected by the electrical activity of the eyes and muscles because of a larger amplitude compared to the true, underlying EEG signal, it is important to reduce the noise from the data. The data we were given was in the time domain, so we converted to the frequency domain using a Fast Fourier Transform. We then applied winsorizing which is a form of clipping to reduce the impact of outliers in the 0-5 and 95-100 percentile range through a 5 percent winsorize operation. 
A second insight we gained was the efficacy of hybrid architectures that utilized both convolutional and recurrent layers. As mentioned before in the introduction, we concluded that only using recurrent layers was not effective. Since a lot of paper mentioned the effectiveness of convolutional neural networks for EEG decoding, we knew that using CNNs in our architecture would be wise. However, recurrent connections are an inherent part of the brain and nervous system and coupling recurrent layers with convolutional layers seemed like a way to capture the temporal relationships in the data. Convolutional networks are good at dealing with variations in the frequency and spatial domain since we have 22 electrodes capturing data so combining CNNs and RNNs can capture the best of both spatial and temporal worlds. 


