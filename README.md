# EEGDecoder
Used Deep neural networks to decode EEG signals to movement-related information

Problem description 
-------------------
The goal of this project was to decode EEG signals into movement-related information through deep learning architectures. Various architectures were investigated using Keras that utilized convolutional and recurrent layers. Through extensive testing, we reached the conclusion that a mixture of convolutional and recurrent layers performed the best and that preprocessing the raw EEG signals was crucial for performance. 

Introduction
------------

Upon tackling the problem, our first idea was to solely use recurrent layers due to the temporal nature of the data. As discussed in class, traditional deep neural nets like convolutional nets and fully connected natures cannot capture temporal relationships in data. As EEG is a time series, we thought it would be tremendously helpful to consider the time sequence information of the signal. However, upon testing using only recurrent layer, our performance was poor as can be seen in Figure x on page x. Our testing for recurrent layers consisted of first using single layer GRUs and LSTMs. The paper "Speech Recognition with Deep Recurrent Neural Networks" by Graves where it was found that the depth of the RNN was more important than the number of cells led us to try stacking recurrent layers. However, the results were still middling and further research and literature review affirmed our belief that RNNs were not the best algorithm for this task as there were few research papers applying RNNs to EEG decoding. On the other hand, there were a plethora of papers that applied deep convolutional networks to the task. Our approach then shifted to focusing on deep convolutional neural network architectures. However, we still wanted to model the recurrent connections that are an inherent part of biological neurons. CNNs are a feed-forward architecture but the time series nature of EEG signals motivated us to try an architecture with a CNN layer followed by an RNN layer. In particular, the paper, "Learning Representations from EEG with Deep-Recurrent-Convolutional Neural Networks," by Dr. Bashivan of the University of Memphis served as an inspiration as tried to design our hybrid architecture. The reasoning behind hybrid architectures is that the ConvNets can deal with variations in space and frequency domains due to their ability to learn two-dimensional representations of the data. These extracted representations could then be fed into a recurrent layer to account for the temporal variations in the data and appropriately model the temporal evolution of brain activity. For the recurrent layer, we decided to use an LSTM as it is the best at capturing long-term dependencies and performed better than GRUs in our testing. The best model we came up with is shown below. It consists of a 1D Convolution with 256 filters of size 3, Max Pooling of size 3, two LSTM layers of 64 units, Gaussian Noise, Dropout of 0.5, and a fully connected output layer of four units with softmax activation. The Gaussian Noise and dropout were added to prevent overfitting by adding robustness to the model while the max pooling was used for down sampling and reducing the number of parameters. 


